################################
# auxiliary function: calculates Huber-weights
# input: 
# x: centered and standardized observations (as vector)
# k: tuning parameter k=1.37 results in efficiency of 0.95 under normal distribution
# output: weights (as vector) based on Huber weight function
################################

whuber <- function(x,k=1.37) return(apply(cbind(1,k/abs(x)),1,min))

################################
# auxiliary function: calculates bisquare-weights
# input:
# x: centered and standardized observations (as vector)
# k: tuning parameter
# output: weights (as vector) based on bisquare weight function
################################

wbi <- function(x,k=1) return(apply(cbind((3*k^4-3*x^2*k^2+x^4)/k^6,1/x^2),1,min))



offdiag <- function (A, at = 0) {
    if (is.matrix(A)) {
        y <- A[row(A) == col(A) - at]
        return(y)
    }
    else {
        len <- length(A)
        B <- matrix(0, nrow = len + abs(at), ncol = len + abs(at))
        B[row(B) == col(B) - at] <- A
        return(B)
    }
}

#######################################
# auxiliary function: estimates simultaneous mean and scale by M estimation
# input
# x: one dimensional random variables as vector
# delta: tunign parameter for M estimator, determines breakpoint, 0.5 is recommended
# epsilon: accuracy of iterated solution
# maxit: maximal number of iterations of iterative reweighting procedure
# k1: tuning parameter for huber weights
# k2: tuning parameter for bisquare weights
# kon: consistency-correction under normal distribution, can be calculated by concorf
# output
# meanv: estimated mean
# sigv: estimated standard deviation
########################################


simul <- function(x,delta=1/2,maxit=10^3,epsilon=10^(-4),k1=1.37,k2=1,kon) {
n <- length(x)

# calculating start values
meanv <- mean(x)
sigv <- mad(x)

# simultaneous M-estimation of location and scale using an iterative reweighting procedure
for (i in (1:maxit)) {
	if(sigv==0) {warning("estimated variance is 0")
        	return(c(meanv,sigv))
		}
	resi <- (x-meanv)/sigv
	we <- whuber(resi,k1)		# Huberweights for location
	meanvn <- sum(we*x)/sum(we)
	resi <- resi/kon
	we <- wbi(resi,k2)		# bisquareweights for scale
	sigvn <- sqrt(sigv^2/n/delta*sum(resi^2*we))
	if((abs(meanvn-meanv)<epsilon*sigv)&(abs(sigvn/sigv-1)<epsilon)) break	# stopping rule
	sigv <- sigvn
	meanv <- meanvn
	if(i==maxit) warning("Iteration may not be converged")
	}
return(c(meanvn,sigvn))
}

#######################################
# auxiliary function: estimates simultaneous one dimensional regressioncoefficient and scale by M estimation
# input
# x: one dimensional independent variable as vector
# y: one dimensional dependent variable as vector
# weightx: weights (depending on Malahanobisdistances of x), generated by bestAR
# delta: tunign parameter for M estimator, determines breakpoint, 0.5 is recommended
# epsilon: accuracy of iterated solution
# maxit: maximal number of iterations of iterative reweighting procedure
# k1: tuning parameter for huber weights
# k2: tuning parameter for bisquare weights
# kon: consistency-correction under normal distribution, can be calculated by concorf
# output
# beta: regressio coefficient
# sigv: estimated standard deviation
########################################


simul3 <- function(y,x,weightx,delta=1/2,maxit=10^3,epsilon=10^(-4),k1=1.37,k2=1,kon) {
n <- length(x)

# calculate start values
erg0 <- ltsReg(y~x-1)
beta <- erg0$coefficients
sigv <- erg0$scale

# simultaneous M-estimation of regression and scale using an iterative reweighting procedure
for (i in (1:maxit)) {
	if(sigv==0) {warning("estimated variance is 0")
        	return(c(beta,sigv))
		}
	resi <- (y-x*beta)/sigv
	weightres <- whuber(resi,k1)	# Huberweights for Regression
	beta <- sum(weightres*weightx*x*y)/sum(weightres*weightx*x^2)	# Mallows-Type
	resit <- resi/kon
	we <- wbi(resit,k2)			# bisquareweights for scale
	sigvn <- sqrt(sigv^2/n/delta*sum(resit^2*we))
	resi2 <- (y-x*beta)/sigvn
if(max(abs(resi-resi2))<epsilon) break	# stopping rule
if(i==maxit) warning("Iteration may not be converged")
sigv <- sigvn
resi <- resi2
}
return(c(beta,sigvn))
}

###################################
# calculates AR-fits using Generalized M estimates (see Robust Statistics, Maronna et al. chapter 8) of increasing order (up to maxp) and returns robust aic values
# input
# timeseries: timeseries of interest as vector
# maxp: maximal order of AR-process
# maxit: maximal number of iterations for iterative weighting procedures of M-estimators
# epsilon: accuracy of iterated solution of M-estimators
# k1: tuning parameter for huber weights
# k2: tuning parameter for bisquare weights
# k3: tuning parameter for regression weights (x-dimension)
# output
# phima: matrix with fitted AR-parameters
# 	 AR modell of order p in row p+1, AR parameter s in column s
# aicv:	robust aic-value of estimated AR processes (in ascending order)
#	smallest value in first element => white noise
#####################################

bestAR <- function(timeseries,maxp,maxit=10^3,delta=1/2,epsilon=10^(-4),k1=1.37,k2=1,k3=1) {
n <- length(timeseries)
if (delta!=1/2) warning("delta of 1/2 is stronlgy recommended. Otherwise breakdownpoint decreases and variance estimation is not consistent.")
# calculating consistency correction
kon <- concorf(k2)

# mean estimation
erg <- simul(timeseries,delta=delta,maxit=maxit,epsilon=epsilon,k1=k1,k2=k2,kon=kon)

phima <- matrix(NA,ncol=maxp,nrow=maxp)
aicv <- rep(NA,maxp+1)
phiacf <- numeric(maxp-1)
sigv <- erg[2]
if (sigv==0) {warning("Estimated variance is 0. Cannot fit AR-modell")
		return(NA)
		}
aicv[1] <- log(sigv^2)
timeseries <- timeseries-erg[1]	# centering

# AR 1 process
weightx <- wbi(timeseries[-n]/sigv,k3)	# weights for Mallows-estimation (x dimension)
erg <- simul3(timeseries[-1],timeseries[-n],weightx=weightx,delta=delta,maxit=maxit,epsilon=epsilon,k1=k1,k2=k2,kon)
phima[1,1] <- erg[1]
phiacf <- erg[1]
aicv[2] <- log(erg[2]^2)+2/(n-1)
if (sigv==0) {warning("Estimated variance is 0. Abort fitting further AR-modells.")
		return(list(phima,aicv))
		}

# AR processes of order > 1
for (p in 2:maxp) {

# using Durbin Levinson to get a one dimensional regression
y <- timeseries[(p+1):n]
x <- timeseries[1:(n-p)]
for (i in 1:(p-1)) {
y <- y-phima[p-1,i]*timeseries[(p+1-i):(n-i)]
x <- x-phima[p-1,p-i]*timeseries[(p+1-i):(n-i)]
}
# calculating the covariance matrix of the (multivariate) independent variables
C <- diag(rep(1,p))
for (i in 1:(p-1)) {
C <- C+offdiag(rep(phiacf[1],p-i),i)
C <- C+offdiag(rep(phiacf[1],p-i),-i)
}
# defining multivariate independent variables to calculate weights of Mallows-type
xma <- matrix(ncol=p,nrow=n-p)
for (i in 1:p) {
xma[,i] <- timeseries[i:(n-p+i-1)]
}
C <- C*sigv
dt <- try(mahalanobis(xma,center=FALSE,cov=C)/p,silent=TRUE) # weights of independent variables
if (inherits(dt,"try-error")) {
	warning("Calculation of Mahalanobisdistances failed. Maybe acf is not positiv definit. Abort fitting further AR-modells.")
	return(list(phima,aicv))	
	}
if (sum(dt<0)>0) {
	warning("Acf is not positiv definit. Abort fitting further AR-modells.")
	return(list(phima,aicv))	
	}
weightx <- wbi(sqrt(dt),k3)
erg <- simul3(y,x,weightx,delta=delta,maxit=maxit,epsilon=epsilon,k1=k1,k2=k2,kon)
beta <- erg[1]
aicv[p+1] <- log(erg[2]^2)+2*p/(n-p)
phima[p,p] <- beta
# updating AR-Parameter by Durbin Levinson
for (i in 1:(p-1)) {
phima[p,i] <- phima[p-1,i]-beta*phima[p-1,p-i]
}
# estimating acf for calculating the covariance matrix of the independent variables
phiacf[p] <- sum(phima[p-1,1:(p-1)]*phiacf[(p-1):1])+beta*(1-sum(phima[p-1,1:(p-1)]*phiacf[1:(p-1)]))
}
phima <- rbind(rep(0,maxp),phima)
rownames(phima) <- paste("AR(",0:maxp,")",sep="")
colnames(phima) <- paste("phi",1:maxp,sep=" ")
names(aicv) <- paste("AR(",0:maxp,")",sep="")
erg <- list(phima,aicv)
names(erg)
return(erg)
}



#######################################
# auxiliary function: calculates consistency corrections for scale M-estimator based on bisquare weights with tuning parameter k
# input: tuning parameter k of bisquare weights 
# output: (simulated) consistency correction factor
#######################################

concorf <- function(x) {

# consistency corrections (first column: k2, second column: consistency factor)

n <- 101
consistcor <- matrix(nrow = n, ncol = 2, dimnames = list(NULL, c("k2", "consistency factors")))
consistcor[, 1] <- exp(0:100 / 10 - 5)
consistcor[, 2] <- 1.54764 / consistcor[, 1]

# if k2 is not in the usual interval

if (x<consistcor[1,1]) {
	warning("k2 is to small, variance estimation will not be consistent")
	kon <- consistcor[1,2]
	}
if (x==consistcor[1,1]) kon <- consistcor[1,2]
if (x==consistcor[n,1]) kon <- consistcor[n,2]
if (x>consistcor[n,1]) {
	warning("k2 is very large, variance estimation might not be consistent")
	kon <- consistcor[n,2]
	}

# linear Interpolation
if ((consistcor[1,1] < x)&( x < consistcor[n,1])) {
	index <- max(which(x>consistcor[,1]))
	kon <- consistcor[index,2]+(consistcor[index+1,2]-consistcor[index,2])/(consistcor[index+1,1]-consistcor[index,1])*(x-consistcor[index,1])
	}
return(kon)
}

 


