################################
# auxiliary function: calculates Huber-weights
# input: 
# x: centered and standardized observations (as vector)
# k: tuning parameter k=1.37 results in efficiency of 0.95 under normal distribution
# output: weights (as vector) based on Huber weight function
################################

whuber <- function(x,k=1.37) return(apply(cbind(1,k/abs(x)),1,min))

################################
# auxiliary function: calculates bisquare-weights
# input:
# x: centered and standardized observations (as vector)
# k: tuning parameter
# output: weights (as vector) based on bisquare weight function
################################

wbi <- function(x,k=1) return(apply(cbind((3*k^4-3*x^2*k^2+x^4)/k^6,1/x^2),1,min))



offdiag <- function (A, at = 0) {
    if (is.matrix(A)) {
        y <- A[row(A) == col(A) - at]
        return(y)
    }
    else {
        len <- length(A)
        B <- matrix(0, nrow = len + abs(at), ncol = len + abs(at))
        B[row(B) == col(B) - at] <- A
        return(B)
    }
}

#######################################
# auxiliary function: estimates simultaneous mean and scale by M estimation
# input
# x: one dimensional random variables as vector
# delta: tunign parameter for M estimator, determines breakpoint, 0.5 is recommended
# epsilon: accuracy of iterated solution
# maxit: maximal number of iterations of iterative reweighting procedure
# k1: tuning parameter for huber weights
# kon: consistency-correction under normal distribution, can be calculated by concorf
# output
# meanv: estimated mean
# sigv: estimated standard deviation
# residuals: x-meanv
########################################


simul <- function(x,delta=1/2,maxit=10^3,epsilon=10^(-4),k1=1.37,kon=concorf(delta)) {
n <- length(x)

# is delta valid?
if (delta <= 0) {delta <- 0.01
warning("Delta <= 0 is not possible. Delta is set to 0.01.")
}
if (delta >= 1) {delta <- 0.5
warning("Delta >= 1 is not possible. Delta is set to 0.5.")
}


# calculating start values
meanv <- mean(x)
sigv <- mad(x)*kon

# simultaneous M-estimation of location and scale using an iterative reweighting procedure
for (i in (1:maxit)) {
	if(sigv==0) {warning("estimated variance is 0")
        	return(c(meanv,sigv))
		}
	resi <- (x-meanv)/sigv
	we <- whuber(resi,k1)		# Huberweights for location
	meanvn <- sum(we*x)/sum(we)
	resi <- resi/1.54764
	we <- wbi(resi,1)		# bisquareweights for scale
	sigvn <- sqrt(sigv^2/n/delta*sum(resi^2*we))
	if((abs(meanvn-meanv)<epsilon*sigv)&(abs(sigvn/sigv-1)<epsilon)) break	# stopping rule
	sigv <- sigvn
	meanv <- meanvn
	if(i==maxit) warning("Iteration may not be converged")
	}
erg <- list(x.mean=meanvn,est.sig=sigvn/kon,residuals=resi)
return(erg)
}


#######################################
# auxiliary function: estimates simultaneous one dimensional regressioncoefficient and scale by M estimation
# input
# x: one dimensional independent variable as vector
# y: one dimensional dependent variable as vector
# weightx: weights (depending on Malahanobisdistances of x), generated by bestAR
# delta: tunign parameter for M estimator, determines breakpoint, 0.5 is recommended
# epsilon: accuracy of iterated solution
# maxit: maximal number of iterations of iterative reweighting procedure
# k1: tuning parameter for huber weights
# kon: consistency-correction under normal distribution, can be calculated by concorf
# output
# beta: regressio coefficient
# sigv: estimated standard deviation
########################################


simul3 <- function(y,x,weightx,delta=1/2,maxit=10^3,epsilon=10^(-4),k1=1.37,kon=concorf(delta)) {
n <- length(x)

# is delta valid?
if (delta <= 0) {delta <- 0.01
warning("Delta <= 0 is not possible. Delta is set to 0.01.")
}
if (delta >= 1) {delta <- 0.5
warning("Delta >= 1 is not possible. Delta is set to 0.5.")
}

# calculate start values
erg0 <- ltsReg(y~x-1)
beta <- erg0$coefficients
sigv <- erg0$scale*kon

# simultaneous M-estimation of regression and scale using an iterative reweighting procedure
for (i in (1:maxit)) {
	if(sigv==0) {warning("estimated variance is 0")
        	return(c(beta,sigv))
		}
	resi <- (y-x*beta)/sigv
	weightres <- whuber(resi,k1)	# Huberweights for Regression
	beta <- sum(weightres*weightx*x*y)/sum(weightres*weightx*x^2)	# Mallows-Type
	resit <- resi/1.54764
	we <- wbi(resit,1)			# bisquareweights for scale
	sigvn <- sqrt(sigv^2/n/delta*sum(resit^2*we))
	resi2 <- (y-x*beta)/sigvn
if(max(abs(resi-resi2))<epsilon) break	# stopping rule
if(i==maxit) warning("Iteration may not be converged")
sigv <- sigvn
resi <- resi2
}
erg <- list(beta=beta,est.sig=sigvn/kon,residuals=resi2)
return(erg)
}

###################################
# calculates AR-fits using Generalized M estimates (see Robust Statistics, Maronna et al. chapter 8) of increasing order (up to maxp) and returns robust aic values
# input
# timeseries: timeseries of interest as vector
# maxp: maximal order of AR-process
# maxit: maximal number of iterations for iterative weighting procedures of M-estimators
# epsilon: accuracy of iterated solution of M-estimators
# k1: tuning parameter for huber weights
# k2: tuning parameter for regressor
# output
# phima: matrix with fitted AR-parameters
# 	 AR modell of order p in row p+1, AR parameter s in column s
# aicv:	robust aic-value of estimated AR processes (in ascending order)
#	smallest value in first element => white noise
#####################################

bestAR <- function(timeseries,maxp,maxit=10^3,delta=1/2,epsilon=10^(-4),k1=1.37,k2=1,aicpenalty=function(p) {return(2*p)}) {
n <- length(timeseries)

# is delta valid?
if (delta <= 0) {delta <- 0.01
warning("Delta <= 0 is not possible. Delta is set to 0.01.")
}
if (delta >= 1) {delta <- 0.5
warning("Delta >= 1 is not possible. Delta is set to 0.5.")
}

# calculating consistency correction
kon <- concorf(delta)
sd.pred <- numeric(maxp+1)
residuals <- matrix(ncol=maxp+1,nrow=n)

# mean estimation
erg <- simul(timeseries,delta=delta,maxit=maxit,epsilon=epsilon,k1=k1,kon=kon)

x.mean <- erg$x.mean
sd.pred[1] <- erg$est.sig
residuals[,1] <- erg$residuals*sd.pred[1]
phima <- matrix(NA,ncol=maxp,nrow=maxp)
aicv <- rep(NA,maxp+1)
phiacf <- numeric(maxp-1)
if (sd.pred[1]==0) {warning("Estimated variance is 0. Cannot fit AR-modell")
		return(NA)
		}
aicv[1] <- log(sd.pred[1]^2)+aicpenalty(1)/n
timeseries <- timeseries-x.mean	# centering

# AR 1 process
weightx <- wbi(timeseries[-n]/sd.pred[1],k2)	# weights for Mallows-estimation (x dimension)
erg <- simul3(timeseries[-1],timeseries[-n],weightx=weightx,delta=delta,maxit=maxit,epsilon=epsilon,k1=k1,kon)
sd.pred[2] <- erg$est.sig
phima[1,1] <- erg$beta
residuals[2:n,2] <- erg$residuals*sd.pred[2]

phiacf <- erg$beta
aicv[2] <- log(sd.pred[2]^2)+aicpenalty(2)/(n-1)
if (sd.pred[2]==0) {warning("Estimated variance is 0. Abort fitting further AR-modells.")
		return(list(phimatrix=phima,aic=aicv,var.pred=sd.pred^2,x.mean=x.mean,residuals=residuals))
		}

# AR processes of order > 1
for (p in 2:maxp) {

# using Durbin Levinson to get a one dimensional regression
y <- timeseries[(p+1):n]
x <- timeseries[1:(n-p)]
for (i in 1:(p-1)) {
y <- y-phima[p-1,i]*timeseries[(p+1-i):(n-i)]
x <- x-phima[p-1,p-i]*timeseries[(p+1-i):(n-i)]
}
# calculating the covariance matrix of the (multivariate) independent variables
C <- diag(rep(1,p))
for (i in 1:(p-1)) {
C <- C+offdiag(rep(phiacf[i],p-i),i)
C <- C+offdiag(rep(phiacf[i],p-i),-i)
}
# defining multivariate independent variables to calculate weights of Mallows-type
xma <- matrix(ncol=p,nrow=n-p)
for (i in 1:p) {
xma[,i] <- timeseries[i:(n-p+i-1)]
}
C <- C*sd.pred[1]^2
dt <- try(mahalanobis(xma,center=FALSE,cov=C)/p,silent=TRUE) # weights of independent variables
if (inherits(dt,"try-error")) {
	warning("Calculation of Mahalanobisdistances failed. Maybe acf is not positiv definit. Abort fitting further AR-modells.")
	phima <- rbind(rep(NA,maxp),phima)
	rownames(phima) <- paste("AR(",0:maxp,")",sep="")
	colnames(phima) <- paste("phi",1:maxp,sep=" ")
	names(aicv) <- paste("AR(",0:maxp,")",sep="")
	return(list(phimatrix=phima,aic=aicv,var.pred=sd.pred^2,x.mean=x.mean,residuals=residuals))	
	}
if (sum(dt<0)>0) {
	warning("Acf is not positiv definit. Abort fitting further AR-modells.")
	phima <- rbind(rep(NA,maxp),phima)
	rownames(phima) <- paste("AR(",0:maxp,")",sep="")
	colnames(phima) <- paste("phi",1:maxp,sep=" ")
	names(aicv) <- paste("AR(",0:maxp,")",sep="")
	return(list(phimatrix=phima,aic=aicv,var.pred=sd.pred^2,x.mean=x.mean,residuals=residuals))	
	}
weightx <- wbi(sqrt(dt),k2)
erg <- simul3(y,x,weightx,delta=delta,maxit=maxit,epsilon=epsilon,k1=k1,kon)
beta <- erg$beta
sd.pred[p+1] <- erg$est.sig
residuals[(p+1):n,p+1] <- erg$residuals*sd.pred[p+1]
aicv[p+1] <- log(sd.pred[p+1]^2)+aicpenalty(p+1)/(n-p)
phima[p,p] <- beta
# updating AR-Parameter by Durbin Levinson
for (i in 1:(p-1)) {
phima[p,i] <- phima[p-1,i]-beta*phima[p-1,p-i]
}
# estimating acf for calculating the covariance matrix of the independent variables
phiacf[p] <- sum(phima[p-1,1:(p-1)]*phiacf[(p-1):1])+beta*(1-sum(phima[p-1,1:(p-1)]*phiacf[1:(p-1)]))
}
phima <- rbind(rep(NA,maxp),phima)
rownames(phima) <- paste("AR(",0:maxp,")",sep="")
colnames(phima) <- paste("phi",1:maxp,sep=" ")
names(aicv) <- paste("AR(",0:maxp,")",sep="")
erg <- list(phimatrix=phima,aic=aicv,var.pred=sd.pred^2,x.mean=x.mean,residuals=residuals)
names(erg)
return(erg)
}


#######################################
# auxiliary function: calculates consistency corrections for scale M-estimator based on bisquare weights with tuning parameter k
# input: tuning parameter k of bisquare weights 
# output: consistency correction factor
#######################################

concorf <- function(x) {

# loading consistency corrections (first column: delta, second column: consistency factor)
corvalues <- get(load(system.file("extdata", "deltacorrection", package = "robts")))
n <- length(corvalues[,1])

# if delta is not in the usual interval

if (x<corvalues[1,1]) {
	warning("delta is to small, variance estimation will not be consistent")
	kon <- corvalues[1,2]
	}
if (x==corvalues[1,1]) kon <- corvalues[1,2]
if (x==corvalues[n,1]) kon <- corvalues[n,2]
if (x>corvalues[n,1]) {
	warning("delta is very large, variance estimation might not be consistent")
	kon <- corvalues[n,2]
	}

# linear Interpolation
if ((corvalues[1,1] < x)&( x < corvalues[n,1])) {
	index <- max(which(x>corvalues[,1]))
	kon <- corvalues[index,2]+(corvalues[index+1,2]-corvalues[index,2])/(corvalues[index+1,1]-corvalues[index,1])*(x-corvalues[index,1])
	}
return(kon)
}
 


